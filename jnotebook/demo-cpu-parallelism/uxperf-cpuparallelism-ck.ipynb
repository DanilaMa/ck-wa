{
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.4.3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "file_extension": ".py",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''<script>\n",
        "code_show=true; \n",
        "function code_toggle() {\n",
        " if (code_show){\n",
        " $('div.input').hide();\n",
        " } else {\n",
        " $('div.input').show();\n",
        " }\n",
        " code_show = !code_show\n",
        "} \n",
        "$( document ).ready(code_toggle);\n",
        "</script>\n",
        "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Import Collective Knowledge (http://github.com/ctuning/ck)\n",
        "import ck.kernel as ck\n",
        "print (ck.__version__)"
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "__version__ = '4.2'\n",
        "\n",
        "from IPython.display import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas\n",
        "import numpy\n",
        "import collections\n",
        "import enum\n",
        "import re\n"
      ],
      "execution_count": 3
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# UXPERF - CPUParallelism\n",
        "\n",
        "###### Requires wa result_processor to have 'cpustates' enabled\n",
        "\n",
        "Detailed breakdown of Parallelism of the device\n",
        "\n",
        "CPU Parallelism - Per Cluster\n",
        "* A Chart per `workload` per `device`\n",
        "* Chart represents per cluster the % time spent with N cores active,\n",
        "  where N is the number of cores. 0 is IDLE/OFF\n",
        "\n",
        "All averaging is defined in `AVG_METHODS`.\n",
        "Valid methods are `median` and `mean`.\n",
        "\n",
        "* `workload` = the wa workload provided in the agenda file\n",
        "* `device` = the physical device the workload was ran on"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# BASE_DIR is the base directory containing the wa data to process.\n",
        "# It is expected to be in the following structure:\n",
        "# BASE_DIR/\n",
        "#     [1 or more] WORKLOAD_NAME/\n",
        "#         [1 or more] DEVICE_NAME/\n",
        "#             [detected by a folder called `__meta`] WA_DATA\n",
        "#\n",
        "# This follows the structure from wa-per-device.py script v2.0 onwards\n",
        "#\n",
        "# To generate new results, you should only need to change this line\n",
        "BASE_DIR = ''\n",
        "\n",
        "# List of averaging methods available\n",
        "AVG_METHODS = enum.Enum('AVG_METHODS', 'MEDIAN MEAN')\n",
        "# Set which averaging method to use here\n",
        "avg_method = AVG_METHODS.MEDIAN"
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### CHART FORMATTING CODE ###\n",
        "\n",
        "# Font and Dimensions\n",
        "font_size = {'S': 14, 'M': 18, 'L': 22} # S=ticks, M=everything else, L=title\n",
        "seaborn.set_context(\"notebook\", rc={\"figure.figsize\" : [16,9]})\n",
        "seaborn.set_style('whitegrid', {\"font.family\" : [\"Gill Sans MT\"]})\n",
        "\n",
        "# ARM Template Colour Scheme\n",
        "arm_colour = (18, 140, 171)\n",
        "arm_green = (0, 169, 96)\n",
        "arm_blue = (0, 195, 220)\n",
        "arm_purple = (118, 95, 151)\n",
        "arm_red = (207, 54, 74)\n",
        "arm_grey = (144, 147, 147)\n",
        "\n",
        "# Lambda Functions\n",
        "rgb_to_hex = lambda rgb: '#'+format(rgb[0]<<16 | rgb[1]<<8 | rgb[2], '06x')\n",
        "\n",
        "# Give a temperature based colour palette\n",
        "# n_colors = Number of colours in the returned palette\n",
        "# grey_end = Make sure the last colour is grey in the palette\n",
        "def temperature_palette(n_colors, grey_end=True):\n",
        "    if grey_end: \n",
        "        n_colors -= 1\n",
        "    palette = seaborn.color_palette(\"RdYlGn\", n_colors=n_colors)\n",
        "    palette.reverse()\n",
        "    if grey_end:\n",
        "        palette.append(rgb_to_hex(arm_grey))\n",
        "    return palette\n",
        "\n",
        "# Using a single colour, generate a palette that scales the tint for n_colors\n",
        "# n_colors = Number of colours for the tinted colours\n",
        "# grey_end = Adds an additional colour, grey, to the end of the palette\n",
        "# grey_start = Adds an additional colour, grey, to the start of the palette\n",
        "def cluster_palette(color, n_colors, grey_end=True, grey_start=False):\n",
        "    tint = lambda rgb, scale: ( int(rgb[0] + (255-rgb[0])*scale),\n",
        "                                int(rgb[1] + (255-rgb[1])*scale),\n",
        "                                int(rgb[2] + (255-rgb[2])*scale))\n",
        "    palette = []\n",
        "    col_count = n_colors\n",
        "    if grey_start: \n",
        "        col_count -= 1\n",
        "        palette.append(rgb_to_hex(arm_grey))\n",
        "    if grey_end:\n",
        "        col_count -= 1\n",
        "    for i in range(n_colors):\n",
        "        tmp = tint(color, float(i)/n_colors)\n",
        "        palette.append(rgb_to_hex(tmp))\n",
        "    if grey_end:\n",
        "        palette.append(rgb_to_hex(arm_grey))\n",
        "    return seaborn.color_palette(palette, n_colors=n_colors)\n",
        "\n",
        "# Defines the formatting for plotting charts\n",
        "# title = The title of the chart\n",
        "# xlable = the xlabel of the chart\n",
        "# ylabel = the ylabel of the chart\n",
        "# xrot = rotation of the xticks\n",
        "# ncol = number of columns for the legend to use\n",
        "# legtitle = the legend title\n",
        "# revleg_ax = reverse the legend. To enable this, you must pass in the axis of the chart to reverse\n",
        "def arm_plot_formatting(title='', xlabel='', ylabel='', xrot=0, ncol=1, legtitle='', revleg_ax=None):\n",
        "    if revleg_ax:\n",
        "        h, l = revleg_ax.get_legend_handles_labels()\n",
        "        revleg_ax.legend(reversed(h), reversed(l),\n",
        "                         title=legtitle, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=font_size['M'], ncol=ncol)\n",
        "    else:\n",
        "        plt.legend(title=legtitle, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=font_size['M'], ncol=ncol)\n",
        "    plt.tick_params(labelsize=font_size['S'])\n",
        "    plt.xticks(rotation=xrot)\n",
        "    plt.title(title+'\\n', fontsize=font_size['L'])\n",
        "    plt.xlabel('\\n'+xlabel, fontsize=font_size['M'], rotation=0, ha='center')\n",
        "    plt.ylabel(ylabel+'\\n', fontsize=font_size['M'], ha='center', va='center')\n",
        "    \n",
        "# Save the current plot to a file.\n",
        "# TAG: To denote the type of chart generated\n",
        "# ARGS: List of arguments to be appended to the image name\n",
        "# This list will be hyphen separated and a PNG will be produced in the BASE_DIR directory\n",
        "def arm_plot_save(tag, args):\n",
        "    if not isinstance(args, list):\n",
        "        args = [args]\n",
        "    args = list(filter(None, args))\n",
        "    fname = '-'.join([tag]+args)+'.png'\n",
        "    plt.savefig(os.path.join(BASE_DIR, fname), bbox_inches='tight')\n",
        "\n",
        "# Setup the Colour Palette using the ARM Template colours\n",
        "arm_palette = seaborn.color_palette([rgb_to_hex(arm_colour), rgb_to_hex(arm_green), rgb_to_hex(arm_blue),\n",
        "                                     rgb_to_hex(arm_purple), rgb_to_hex(arm_red), rgb_to_hex(arm_grey)], n_colors=6)\n",
        "seaborn.set_palette(arm_palette)"
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### FUNCTIONS ###\n",
        "# common to all uxperf scripts #\n",
        "\n",
        "# Lambda Functions\n",
        "dropzero = lambda x: x[(x.T != 0).any()]\n",
        "normalise_data = lambda x: x-x.iloc[0,0]\n",
        "\n",
        "# Parse the meta folder looking for the cluster information in the json file\n",
        "def get_clusters(file_path):\n",
        "    json_file = ''\n",
        "    for f in os.listdir(file_path):\n",
        "        if f.endswith('json'):\n",
        "            json_file = os.path.join(file_path, f)\n",
        "            break\n",
        "    if os.path.isfile(json_file):\n",
        "        with open(json_file, 'r') as j:\n",
        "            data = json.load(j)\n",
        "        try:\n",
        "            clusters = data['device_config']['core_clusters']\n",
        "            print ('Number of clusters:', len(set(clusters)))\n",
        "        except:\n",
        "            print ('Unable to find core_clusters information... Will assume one cluster...')\n",
        "            clusters = None\n",
        "    else:\n",
        "        print ('Error could not find file:', file_path)\n",
        "        clusters = None\n",
        "    return clusters\n",
        "\n",
        "# Parse status.txt for iterations that completed successfully and append its index to a list\n",
        "def get_iterations(file_path):\n",
        "    if os.path.isfile(file_path):\n",
        "        iterations = []\n",
        "        with open(file_path, 'r') as status:\n",
        "            for line in status:\n",
        "                if ('OK' in line) and ('Run status' not in line):\n",
        "                    iteration = int(list(filter(None, line.strip().split(' ')))[2])\n",
        "                    iterations.append(iteration)\n",
        "        return iterations\n",
        "    else:\n",
        "        print ('Error could not find file:', file_path)\n",
        "        return None\n",
        "\n",
        "# Load the csv file and optionally drop unneccessary columns\n",
        "def load_csv(file_path, drop=None):\n",
        "    if os.path.isfile(file_path):\n",
        "        data = pandas.read_csv(file_path)\n",
        "        if drop:\n",
        "            data = data.drop(drop, axis=1)\n",
        "        return data\n",
        "    else:\n",
        "        print ('Error could not find file:', file_path)\n",
        "        return None\n",
        "    \n",
        "# Only keep results in data from successful iterations\n",
        "def process_iterations(data, iterations):\n",
        "    if (data is not None) and (iterations is not None):\n",
        "        return data[data['iteration'].map(lambda x: x in iterations)]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Group and perform averaging on the results. Drop iteration column\n",
        "def avg_iterations_by(data, group):\n",
        "    if data is not None:\n",
        "        groups = data.groupby(group, sort=False)\n",
        "        if avg_method == AVG_METHODS.MEDIAN:\n",
        "            avg = groups.median()\n",
        "        elif avg_method == AVG_METHODS.MEAN:\n",
        "            avg = groups.mean()\n",
        "        avg = avg.drop('iteration', axis=1)\n",
        "        avg.index.name = None\n",
        "        return avg\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "# Drop rows that contain IDLE state and append a single IDLE state representing them all\n",
        "def merge_idles(data):\n",
        "    if data is not None:\n",
        "        noidle = data[~data.index.str.contains('MWAIT|CPUIDLE|WFI|sleep')]\n",
        "        idle = pandas.DataFrame([100-x for x in noidle.sum()], index=noidle.columns, columns=['IDLE']).T\n",
        "        return noidle.append(idle)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Convert KHz to MHz and drop the label, in place\n",
        "def khz_to_mhz(data):\n",
        "    if data is not None:\n",
        "        mhz = []\n",
        "        for state in data.index:\n",
        "            if state is not 'IDLE':\n",
        "                khz = int(state.replace('KHz', ''))\n",
        "                mhz.append(khz/1000)\n",
        "            else:\n",
        "                mhz.append(state)\n",
        "        data.index = mhz\n",
        "\n",
        "# For multiple dataframes, perform averaging on their data and concat to one larger dataframe\n",
        "# data_list expects 2 values per item. Device (title) and Data\n",
        "def avg_columns(data_list):\n",
        "    results = None\n",
        "    for device, data in data_list:\n",
        "        if avg_method == AVG_METHODS.MEDIAN:\n",
        "            avg = data.median(axis=1)\n",
        "        elif avg_method == AVG_METHODS.MEAN:\n",
        "            avg = data.mean(axis=1)\n",
        "        avg.name = device\n",
        "        results = avg.to_frame() if results is None else pandas.concat([results, avg], axis=1)\n",
        "    results.fillna(0, inplace=True)\n",
        "    return dropzero(results)"
      ],
      "execution_count": 6
    },
    {
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### FUNCTIONS ###\n",
        "# unique to this uxperf script #"
      ],
      "execution_count": 7
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Process raw input data\n",
        "\n",
        "input: `BASE_DIR`\n",
        "\n",
        "output: `DATA_SET`"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Obtain ALL WA results entries with meta via CK\n",
        "r=ck.access({'action':'search',\n",
        "             'module_uoa':'wa-result',\n",
        "             'add_meta':'yes'})\n",
        "if r['return']>0: ck.jerr(r)\n",
        "lst=r['lst']\n",
        "ck.out('Number of CK entries: '+str(len(lst)))"
      ],
      "execution_count": 10
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#BASE_DIR='.'\n",
        "dir_count = 0\n",
        "DATA_SET = {}\n",
        "\n",
        "for l in lst:\n",
        "    folder=os.path.join(l['path'],'results')\n",
        "    meta=l['meta']['meta']\n",
        "    \n",
        "    workload=meta['workload_name']\n",
        "    device=meta['local_device_uoa']\n",
        "\n",
        "#if os.path.isdir(BASE_DIR):\n",
        "#    for workload in os.listdir(BASE_DIR):\n",
        "#        w_dir = os.path.join(BASE_DIR, workload)\n",
        "#        if os.path.isdir(w_dir):\n",
        "#            for device in os.listdir(w_dir):\n",
        "#                folder = os.path.join(w_dir, device)\n",
        "#                if os.path.isdir(folder):\n",
        "                    # Valid wa data folder\n",
        "    if '__meta' in os.listdir(folder):\n",
        "        print ('Processing folder:', folder)\n",
        "        print ('Workload:', workload)\n",
        "        print ('Device: ', device)\n",
        "        dir_count += 1\n",
        "        iterations = get_iterations(os.path.join(folder, 'status.txt'))\n",
        "        # 1 or more successful iterations\n",
        "        if iterations is not None and len(iterations) > 0:\n",
        "            print ('Number of successful iterations:', len(iterations))\n",
        "            raw_data = load_csv(os.path.join(folder, 'parallel.csv'), ['id', 'workload'])\n",
        "            valid_data = process_iterations(raw_data, iterations)\n",
        "            data = avg_iterations_by(valid_data, ['cluster', u'number_of_cores'])\n",
        "            if data is not None:\n",
        "                if workload not in DATA_SET:\n",
        "                    DATA_SET[workload] = []\n",
        "                DATA_SET[workload].append([device, data])\n",
        "            else:\n",
        "                print ('Warning!!! No data found. Skipping folder...')\n",
        "        else:\n",
        "            print ('Warning!!! No successful iterations found. Skipping folder...')\n",
        "    else:\n",
        "        print ('Warning!!! Not a valid wa data folder. Skipping folder...')\n",
        "    print()\n",
        "print()\n",
        "print ('Processed {} directories'.format(dir_count))\n",
        "print ('Obtained {} data sets'.format(sum([len(DATA_SET[key]) for key in DATA_SET])))"
      ],
      "execution_count": 12
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## CPU Parallelism - Per Cluster\n",
        "\n",
        "input: `DATA_SET`"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plist = []\n",
        "for workload in DATA_SET:\n",
        "    print (workload)\n",
        "    for device, data in DATA_SET[workload]:\n",
        "        pdata = data.reset_index()\n",
        "        clusters = collections.Counter(pdata['cluster'].values).keys()\n",
        "        cluster_count = len(clusters) - 1\n",
        "        pdata = (pdata if (cluster_count > 1) else pdata[pdata['cluster'] == '0'])\n",
        "        print (device)\n",
        "        display(pdata)\n",
        "        plist.append([workload, device, pdata])\n",
        "    print"
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "source": [
        "for workload, device, data in plist:\n",
        "    ncols = pandas.value_counts(data['cluster'].values)[0]\n",
        "    plt.figure()\n",
        "    seaborn.set_palette(cluster_palette(arm_colour, ncols, grey_end=False, grey_start=True))\n",
        "    ax = seaborn.barplot(data=data, x='cluster', y='%time', hue='number_of_cores')\n",
        "    ax.set_ylim(0,100)\n",
        "    arm_plot_formatting(title='CPU Parallelism - Per Cluster\\nNumber of active cores', xlabel='Workload: '+workload, ylabel='Time with N cores active (%)', legtitle=device)\n",
        "    arm_plot_save('CP', [workload, device])"
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "cell_type": "code",
      "source": [],
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "cell_type": "code",
      "source": [],
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "cell_type": "code",
      "source": [],
      "execution_count": null
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}
